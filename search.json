[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "lightning.html",
    "href": "lightning.html",
    "title": "lightning",
    "section": "",
    "text": "Lightning has multiple libraries that trade-off structure+simplicity vs expert-level control.\nHere’s how you can decide what’s important to you:\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torchvision as tv\nimport lightning as L\n\nmodel\n\nencoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\ndecoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\n\n\nencoder\n\nSequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=3, bias=True)\n)\n\n\n\ndecoder\n\nSequential(\n  (0): Linear(in_features=3, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=784, bias=True)\n)\n\n\n\nclass LitAutoEncoder(L.LightningModule):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder, self.decoder = encoder, decoder\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = nn.functional.mse_loss(x_hat, x)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\n\n\n# init model\nmodel = LitAutoEncoder(encoder, decoder)\n\ndata\n\n# get a list of all the available datasets\ndataset_names = tv.datasets.__all__\n\n# print the list of dataset names\nprint(\"Available datasets:\")\nfor name in dataset_names:\n    print(name)\n    # print(f\"{name}: {dataset.__doc__}\")\n\nAvailable datasets:\nLSUN\nLSUNClass\nImageFolder\nDatasetFolder\nFakeData\nCocoCaptions\nCocoDetection\nCIFAR10\nCIFAR100\nEMNIST\nFashionMNIST\nQMNIST\nMNIST\nKMNIST\nStanfordCars\nSTL10\nSUN397\nSVHN\nPhotoTour\nSEMEION\nOmniglot\nSBU\nFlickr8k\nFlickr30k\nFlowers102\nVOCSegmentation\nVOCDetection\nCityscapes\nImageNet\nCaltech101\nCaltech256\nCelebA\nWIDERFace\nSBDataset\nVisionDataset\nUSPS\nKinetics\nHMDB51\nUCF101\nPlaces365\nKitti\nINaturalist\nLFWPeople\nLFWPairs\nKittiFlow\nSintel\nFlyingChairs\nFlyingThings3D\nHD1K\nFood101\nDTD\nFER2013\nGTSRB\nCLEVRClassification\nOxfordIIITPet\nPCAM\nCountry211\nFGVCAircraft\nEuroSAT\nRenderedSST2\nKitti2012Stereo\nKitti2015Stereo\nCarlaStereo\nMiddlebury2014Stereo\nCREStereo\nFallingThingsStereo\nSceneFlowStereo\nSintelStereo\nInStereo2k\nETH3DStereo\nwrap_dataset_for_transforms_v2\n\n\n\n# define the transforms to apply to the data\n\ntransform = tv.transforms.Compose([tv.transforms.ToTensor(),\n                                   tv.transforms.Normalize((0.5,), (0.5,))])\n\n\nmnist_data = tv.datasets.MNIST(\".\", download=True, transform=transform)\n\n\n# split the data into train and validation sets (55000/5000)\nmnist_train, mnist_val = data.random_split(mnist_data, [55000, 5000])\n\n\n# define the dataloaders for the train and validation sets, with a batch size of 64\nmnist_train_loader = data.DataLoader(mnist_train, batch_size=64)\nmnist_val_loader = data.DataLoader(mnist_val, batch_size=64)\n\ntrain\n\n# define trainer\ntrainer = L.Trainer(max_steps=1000)\n\nTrainer will use only 1 of 6 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=6)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/ma/miniconda3/envs/myl/lib/python3.10/site-pac ...\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n\n\ntrainer.fit(model, mnist_train_loader, mnist_val_loader)\n\n/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\nYou are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]\n\n  | Name    | Type       | Params\n---------------------------------------\n0 | encoder | Sequential | 100 K \n1 | decoder | Sequential | 101 K \n---------------------------------------\n202 K     Trainable params\n0         Non-trainable params\n202 K     Total params\n0.810     Total estimated model params size (MB)\n/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=71` in the `DataLoader` to improve performance.\n`Trainer.fit` stopped: `max_steps=1000` reached.\n\n\n\n\n\ndeploy\n\n# load checkpoint\ncheckpoint = \"./lightning_logs/version_5/checkpoints/epoch=1-step=1000.ckpt\"\nautoencoder = LitAutoEncoder.load_from_checkpoint(checkpoint, encoder=encoder, decoder=decoder)\n\n# choose your trained nn.Module\nencoder = autoencoder.encoder\nencoder.eval()\n\n# embed 4 fake images!\nfake_image_batch = torch.Tensor(4, 28 * 28).to(next(encoder.parameters()).device)\nembeddings = encoder(fake_image_batch)\nprint(\"⚡\" * 20, \"\\nPredictions (4 image embeddings):\\n\", embeddings, \"\\n\", \"⚡\" * 20)\n\n⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡ \nPredictions (4 image embeddings):\n tensor([[ 5.6811e+36, -5.2628e+36,  3.0731e+36],\n        [-6.1849e+35, -4.5976e+36,  3.5895e+36],\n        [ 6.4357e+33, -1.0275e+34, -7.8269e+33],\n        [ 9.8027e+32,  1.6378e+32, -1.0713e+33]], device='cuda:0',\n       grad_fn=&lt;AddmmBackward0&gt;) \n ⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡"
  },
  {
    "objectID": "nanogpt.html",
    "href": "nanogpt.html",
    "title": "gpt",
    "section": "",
    "text": "import\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\nimport pytorch_lightning as L \nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndata\nhttps://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\nDATA_FILE = 'data/input.txt'\n\n# read the data file\nwith open(DATA_FILE, 'r') as f:\n    text = f.read()\n\n\n# print the length of the text\nprint('Length of text: {} characters'.format(len(text)))\n\nLength of text: 1115393 characters\n\n\n\n# look at the first 250 characters in text\nprint(text[:250])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\n\n\n\n# The unique characters in the file, as a sorted list.\nvocab = sorted(set(text))\n# print out the vocabulary \nprint('Vocabulary: {}'.format(vocab))\n\n# the number of unique characters\nvocab_size = len(vocab)\n# print the number of unique characters\nprint('Number of unique characters: {}'.format(vocab_size))\n\nVocabulary: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\nNumber of unique characters: 65\n\n\n\n# a mapping from characters to indices\nchar2idx = {u:i for i, u in enumerate(vocab)}\n\n# a mapping from indices to characters\nidx2char = {i:u for i, u in enumerate(vocab)}\n\n\n# an encoder function that converts text to a torch tensor\ndef encoder(text):\n    return torch.tensor([char2idx[c] for c in text], dtype=torch.long)\n\n\nencoder('Hello Olivier!')\n\ntensor([20, 43, 50, 50, 53,  1, 27, 50, 47, 60, 47, 43, 56,  2])\n\n\n\n# a decoder function that converts a torch tensor to text\ndef decoder(tensor):\n    return ''.join([idx2char[i.item()] for i in tensor])\n\n\ndecoder(encoder('Hello Olivier!'))\n\n'Hello Olivier!'\n\n\na bigger vocabulary would normally imply a shorter encoding length\nSentencePiece BPE tiktoken\n\n# encode the whole text\nencoded_text = encoder(text)\n\n# print the encoded text (first 500 characters)\nprint(encoded_text[:500])\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56])\n\n\n\nprint(decoder(encoded_text[:500]))\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor\n\n\n\n# train/validation split (90/10)\ntrain_size = int(0.9 * len(encoded_text))\nval_size = len(encoded_text) - train_size\ntrain_text, val_text = torch.utils.data.random_split(encoded_text, [train_size, val_size])\n\n\n# set the block size to 8 \nblock_size = 8\n\nand visualise how the self-supervised model do its predictions\n\nx = train_text[0:block_size]\ny = train_text[1:block_size+1]\nx, y\n\n(tensor([24, 42, 47, 11, 46, 52, 50, 42]),\n tensor([42, 47, 11, 46, 52, 50, 42, 53]))\n\n\nnext token prediction\nthe model looks at all the previous tokens, up to the length of the block size\n\n# show the context and target for all the training examples\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print('context: {} -&gt; target: {}'.format(context, target))\n\ncontext: tensor([24]) -&gt; target: 42\ncontext: tensor([24, 42]) -&gt; target: 47\ncontext: tensor([24, 42, 47]) -&gt; target: 11\ncontext: tensor([24, 42, 47, 11]) -&gt; target: 46\ncontext: tensor([24, 42, 47, 11, 46]) -&gt; target: 52\ncontext: tensor([24, 42, 47, 11, 46, 52]) -&gt; target: 50\ncontext: tensor([24, 42, 47, 11, 46, 52, 50]) -&gt; target: 42\ncontext: tensor([24, 42, 47, 11, 46, 52, 50, 42]) -&gt; target: 53\n\n\n\nbatch_size = 4\n\nthe dataset class\n\nclass TextDataset(Dataset):\n    def __init__(self, text, block_size=8):\n        self.text = text\n        self.block_size = block_size\n        \n    def __len__(self):\n        return len(self.text) // self.block_size\n    \n    def __getitem__(self, idx):\n        # get a block of size block_size starting from index idx * block_size\n        start_idx = idx * self.block_size\n        end_idx = start_idx + self.block_size\n        data = self.text[start_idx:end_idx]\n        target = self.text[start_idx+1:end_idx+1]\n        return data, target\n\ndataloader\n\n# create separate train and validation datasets\ntrain_dataset = TextDataset(train_text, block_size=block_size)\nval_dataset = TextDataset(val_text, block_size=block_size)\n\n# create separate train and validation dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n\nxb, yb = next(iter(train_dataloader))\nxb, yb\n\n(tensor([[49,  7, 21, 43,  1, 42,  1, 63],\n         [ 1, 58,  1, 41, 19, 59, 52,  8],\n         [56, 53,  1, 44, 16, 53,  1, 52],\n         [ 1, 46, 53, 53, 57, 10, 43, 58]]),\n tensor([[ 7, 21, 43,  1, 42,  1, 63, 30],\n         [58,  1, 41, 19, 59, 52,  8, 58],\n         [53,  1, 44, 16, 53,  1, 52, 28],\n         [46, 53, 53, 57, 10, 43, 58, 56]]))\n\n\n\n# loop through the batches and print the context and target for each batch\nfor b in range(batch_size):\n    print('Batch {}'.format(b+1))\n    for t in range(block_size):\n        context = xb[b, :t+1]\n        target = yb[b, t]\n        print('context: {} -&gt; target: {}'.format(context, target))\n\nBatch 1\ncontext: tensor([49]) -&gt; target: 7\ncontext: tensor([49,  7]) -&gt; target: 21\ncontext: tensor([49,  7, 21]) -&gt; target: 43\ncontext: tensor([49,  7, 21, 43]) -&gt; target: 1\ncontext: tensor([49,  7, 21, 43,  1]) -&gt; target: 42\ncontext: tensor([49,  7, 21, 43,  1, 42]) -&gt; target: 1\ncontext: tensor([49,  7, 21, 43,  1, 42,  1]) -&gt; target: 63\ncontext: tensor([49,  7, 21, 43,  1, 42,  1, 63]) -&gt; target: 30\nBatch 2\ncontext: tensor([1]) -&gt; target: 58\ncontext: tensor([ 1, 58]) -&gt; target: 1\ncontext: tensor([ 1, 58,  1]) -&gt; target: 41\ncontext: tensor([ 1, 58,  1, 41]) -&gt; target: 19\ncontext: tensor([ 1, 58,  1, 41, 19]) -&gt; target: 59\ncontext: tensor([ 1, 58,  1, 41, 19, 59]) -&gt; target: 52\ncontext: tensor([ 1, 58,  1, 41, 19, 59, 52]) -&gt; target: 8\ncontext: tensor([ 1, 58,  1, 41, 19, 59, 52,  8]) -&gt; target: 58\nBatch 3\ncontext: tensor([56]) -&gt; target: 53\ncontext: tensor([56, 53]) -&gt; target: 1\ncontext: tensor([56, 53,  1]) -&gt; target: 44\ncontext: tensor([56, 53,  1, 44]) -&gt; target: 16\ncontext: tensor([56, 53,  1, 44, 16]) -&gt; target: 53\ncontext: tensor([56, 53,  1, 44, 16, 53]) -&gt; target: 1\ncontext: tensor([56, 53,  1, 44, 16, 53,  1]) -&gt; target: 52\ncontext: tensor([56, 53,  1, 44, 16, 53,  1, 52]) -&gt; target: 28\nBatch 4\ncontext: tensor([1]) -&gt; target: 46\ncontext: tensor([ 1, 46]) -&gt; target: 53\ncontext: tensor([ 1, 46, 53]) -&gt; target: 53\ncontext: tensor([ 1, 46, 53, 53]) -&gt; target: 57\ncontext: tensor([ 1, 46, 53, 53, 57]) -&gt; target: 10\ncontext: tensor([ 1, 46, 53, 53, 57, 10]) -&gt; target: 43\ncontext: tensor([ 1, 46, 53, 53, 57, 10, 43]) -&gt; target: 58\ncontext: tensor([ 1, 46, 53, 53, 57, 10, 43, 58]) -&gt; target: 56\n\n\n\n\nBigram model\n\n# define a Bigram language model in PyTorch-Lightning\nclass BigramModel(L.LightningModule):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super().__init__()\n        self.save_hyperparameters()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat.view(-1, self.hparams.vocab_size), y.view(-1))\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat.view(-1, self.hparams.vocab_size), y.view(-1))\n        self.log('val_loss', loss)\n        return loss\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\n    \n    def generate(self, starting_text, length):\n        with torch.no_grad():\n            # convert starting_text to tensor\n            input_seq = encoder(starting_text).unsqueeze(0)\n            # move input_seq to device\n            input_seq = input_seq.to(self.device)\n            # generate sequence of length 'length'\n            for i in range(length):\n                # get output probabilities from model\n                output_probs = self(input_seq)[:,-1,:]\n                # sample the next token from the output probabilities\n                next_token = torch.multinomial(F.softmax(output_probs, dim=-1), num_samples=1)\n                # append the next token to the input sequence\n                input_seq = torch.cat([input_seq, next_token], dim=1)\n        # convert the output sequence to text\n        output_text = ''.join([idx2char[idx] for idx in input_seq.squeeze().tolist()])\n        return output_text\n\ninitialise the model\n\nbigram_model = BigramModel(vocab_size, embedding_dim=32, hidden_dim=64)\nbigram_model\n\nBigramModel(\n  (embedding): Embedding(65, 32)\n  (fc1): Linear(in_features=32, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=65, bias=True)\n)\n\n\nforward pass\n\nbigram_model(xb)\n\ntensor([[[-0.0898,  0.1966, -0.2269,  ...,  0.0039, -0.0763, -0.1268],\n         [ 0.1250, -0.0110,  0.0631,  ...,  0.0591,  0.2566,  0.0475],\n         [-0.0113,  0.0902,  0.1115,  ..., -0.0293,  0.0861,  0.1530],\n         ...,\n         [ 0.0069,  0.1929, -0.0406,  ...,  0.2338, -0.1185, -0.1099],\n         [-0.2324, -0.0831, -0.3524,  ..., -0.2395,  0.1335,  0.0672],\n         [ 0.0411,  0.1028, -0.1139,  ..., -0.1857, -0.0310,  0.0017]],\n\n        [[-0.2324, -0.0831, -0.3524,  ..., -0.2395,  0.1335,  0.0672],\n         [-0.3195,  0.5125, -0.0533,  ...,  0.2048,  0.1758,  0.0952],\n         [-0.2324, -0.0831, -0.3524,  ..., -0.2395,  0.1335,  0.0672],\n         ...,\n         [-0.1921,  0.1451, -0.0171,  ..., -0.1576, -0.1355, -0.0993],\n         [-0.2719,  0.0547, -0.0394,  ...,  0.0629,  0.0370,  0.3403],\n         [-0.2136,  0.0358,  0.0807,  ..., -0.3666,  0.1520, -0.0436]],\n\n        [[-0.4426, -0.0566,  0.0663,  ...,  0.0090,  0.2780,  0.1964],\n         [ 0.0708,  0.2102,  0.3248,  ..., -0.0728, -0.2538,  0.2603],\n         [-0.2324, -0.0831, -0.3524,  ..., -0.2395,  0.1335,  0.0672],\n         ...,\n         [ 0.0708,  0.2102,  0.3248,  ..., -0.0728, -0.2538,  0.2603],\n         [-0.2324, -0.0831, -0.3524,  ..., -0.2395,  0.1335,  0.0672],\n         [-0.2719,  0.0547, -0.0394,  ...,  0.0629,  0.0370,  0.3403]],\n\n        [[-0.2324, -0.0831, -0.3524,  ..., -0.2395,  0.1335,  0.0672],\n         [-0.0486, -0.0988, -0.4984,  ..., -0.4791,  0.1320,  0.1301],\n         [ 0.0708,  0.2102,  0.3248,  ..., -0.0728, -0.2538,  0.2603],\n         ...,\n         [ 0.1366,  0.0522,  0.0452,  ..., -0.0702,  0.1523,  0.3359],\n         [-0.2125,  0.1325,  0.1413,  ..., -0.0728, -0.0152, -0.1315],\n         [-0.3195,  0.5125, -0.0533,  ...,  0.2048,  0.1758,  0.0952]]],\n       grad_fn=&lt;ViewBackward0&gt;)\n\n\ntraining step\n\nbigram_model.training_step((xb, yb), 0)\n\n/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/pytorch_lightning/core/module.py:420: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n\n\ntensor(4.1391, grad_fn=&lt;NllLossBackward0&gt;)\n\n\ngenerate text\n\nbigram_model.generate('Hello', 100)\n\n\"HelloFMPdOlgqbqMfgkh,v':okQqg&$XWIK?TEU.kM Zsoh- tTwkWBg\\ndk;XSoKIkSmiF'DjmE'h,OazprsW!C:mLtgWPnDtnVixaZm\\n\"\n\n\nuse a larger batch size for actual training\n\nbatch_size = 32\n# create separate train and validation dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\ntrainer\n\n# define a trainer object\ntrainer = L.Trainer(max_epochs=1)\n\nTrainer will use only 1 of 6 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=6)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/ma/miniconda3/envs/myl/lib/python3.10/site-pac ...\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n\n\n# train the model\ntrainer.fit(bigram_model, train_dataloader, val_dataloader)\n\nYou are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]\n\n  | Name      | Type      | Params\n----------------------------------------\n0 | embedding | Embedding | 2.1 K \n1 | fc1       | Linear    | 2.1 K \n2 | fc2       | Linear    | 4.2 K \n----------------------------------------\n8.4 K     Trainable params\n0         Non-trainable params\n8.4 K     Total params\n0.034     Total estimated model params size (MB)\n/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=71` in the `DataLoader` to improve performance.\n/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=71` in the `DataLoader` to improve performance.\n`Trainer.fit` stopped: `max_epochs=1` reached.\n\n\n\n\n\n\n\n\n\n\n\nmetrics\n\n# check the training loss\ntrainer.logged_metrics\n\n{'train_loss': tensor(3.2474), 'val_loss': tensor(3.3175)}\n\n\ngenerate\n\ninput_text = \"hello\"\n\n# generate a sequence of length 100\nprint(bigram_model.generate(input_text, 100))\n\nhelloin eenslorE lnln ezln,isya  iaminN faw t\nn sou  nuaDb   crei Iwido i \ne\nyk  ro w  'ItIirhaiiihvet  e\n\n\n\nxb, yb\n\n(tensor([[49,  7, 21, 43,  1, 42,  1, 63],\n         [ 1, 58,  1, 41, 19, 59, 52,  8],\n         [56, 53,  1, 44, 16, 53,  1, 52],\n         [ 1, 46, 53, 53, 57, 10, 43, 58]]),\n tensor([[ 7, 21, 43,  1, 42,  1, 63, 30],\n         [58,  1, 41, 19, 59, 52,  8, 58],\n         [53,  1, 44, 16, 53,  1, 52, 28],\n         [46, 53, 53, 57, 10, 43, 58, 56]]))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myl",
    "section": "",
    "text": "pip install myl"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "myl",
    "section": "",
    "text": "pip install myl"
  }
]