{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt\n",
    "\n",
    "> minimum GPT model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as L \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'data/input.txt'\n",
    "\n",
    "# read the data file\n",
    "with open(DATA_FILE, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115393 characters\n"
     ]
    }
   ],
   "source": [
    "# print the length of the text\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Number of unique characters: 65\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file, as a sorted list.\n",
    "vocab = sorted(set(text))\n",
    "# print out the vocabulary \n",
    "print('Vocabulary: {}'.format(vocab))\n",
    "\n",
    "# the number of unique characters\n",
    "vocab_size = len(vocab)\n",
    "# print the number of unique characters\n",
    "print('Number of unique characters: {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a mapping from characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "# a mapping from indices to characters\n",
    "idx2char = {i:u for i, u in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an encoder function that converts text to a torch tensor\n",
    "def encoder(text):\n",
    "    return torch.tensor([char2idx[c] for c in text], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 43, 50, 50, 53,  1, 27, 50, 47, 60, 47, 43, 56,  2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder('Hello Olivier!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a decoder function that converts a torch tensor to text\n",
    "def decoder(tensor):\n",
    "    return ''.join([idx2char[i.item()] for i in tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Olivier!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(encoder('Hello Olivier!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a bigger vocabulary would normally imply a shorter encoding length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentencePiece BPE tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56])\n"
     ]
    }
   ],
   "source": [
    "# encode the whole text\n",
    "encoded_text = encoder(text)\n",
    "\n",
    "# print the encoded text (first 500 characters)\n",
    "print(encoded_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(decoder(encoded_text[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/validation split (90/10)\n",
    "train_size = int(0.9 * len(encoded_text))\n",
    "val_size = len(encoded_text) - train_size\n",
    "train_text, val_text = torch.utils.data.random_split(encoded_text, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the block size to 8 \n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and visualise how the self-supervised model do its predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([24, 42, 47, 11, 46, 52, 50, 42]),\n",
       " tensor([42, 47, 11, 46, 52, 50, 42, 53]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_text[0:block_size]\n",
    "y = train_text[1:block_size+1]\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next token prediction\n",
    "\n",
    "the model looks at all the previous tokens, up to the length of the block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: tensor([24]) -> target: 42\n",
      "context: tensor([24, 42]) -> target: 47\n",
      "context: tensor([24, 42, 47]) -> target: 11\n",
      "context: tensor([24, 42, 47, 11]) -> target: 46\n",
      "context: tensor([24, 42, 47, 11, 46]) -> target: 52\n",
      "context: tensor([24, 42, 47, 11, 46, 52]) -> target: 50\n",
      "context: tensor([24, 42, 47, 11, 46, 52, 50]) -> target: 42\n",
      "context: tensor([24, 42, 47, 11, 46, 52, 50, 42]) -> target: 53\n"
     ]
    }
   ],
   "source": [
    "# show the context and target for all the training examples\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print('context: {} -> target: {}'.format(context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, block_size=8):\n",
    "        self.text = text\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text) // self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get a block of size block_size starting from index idx * block_size\n",
    "        start_idx = idx * self.block_size\n",
    "        end_idx = start_idx + self.block_size\n",
    "        data = self.text[start_idx:end_idx]\n",
    "        target = self.text[start_idx+1:end_idx+1]\n",
    "        return data, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate train and validation datasets\n",
    "train_dataset = TextDataset(train_text, block_size=block_size)\n",
    "val_dataset = TextDataset(val_text, block_size=block_size)\n",
    "\n",
    "# create separate train and validation dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[49,  7, 21, 43,  1, 42,  1, 63],\n",
       "         [ 1, 58,  1, 41, 19, 59, 52,  8],\n",
       "         [56, 53,  1, 44, 16, 53,  1, 52],\n",
       "         [ 1, 46, 53, 53, 57, 10, 43, 58]]),\n",
       " tensor([[ 7, 21, 43,  1, 42,  1, 63, 30],\n",
       "         [58,  1, 41, 19, 59, 52,  8, 58],\n",
       "         [53,  1, 44, 16, 53,  1, 52, 28],\n",
       "         [46, 53, 53, 57, 10, 43, 58, 56]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dataloader))\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "context: tensor([49]) -> target: 7\n",
      "context: tensor([49,  7]) -> target: 21\n",
      "context: tensor([49,  7, 21]) -> target: 43\n",
      "context: tensor([49,  7, 21, 43]) -> target: 1\n",
      "context: tensor([49,  7, 21, 43,  1]) -> target: 42\n",
      "context: tensor([49,  7, 21, 43,  1, 42]) -> target: 1\n",
      "context: tensor([49,  7, 21, 43,  1, 42,  1]) -> target: 63\n",
      "context: tensor([49,  7, 21, 43,  1, 42,  1, 63]) -> target: 30\n",
      "Batch 2\n",
      "context: tensor([1]) -> target: 58\n",
      "context: tensor([ 1, 58]) -> target: 1\n",
      "context: tensor([ 1, 58,  1]) -> target: 41\n",
      "context: tensor([ 1, 58,  1, 41]) -> target: 19\n",
      "context: tensor([ 1, 58,  1, 41, 19]) -> target: 59\n",
      "context: tensor([ 1, 58,  1, 41, 19, 59]) -> target: 52\n",
      "context: tensor([ 1, 58,  1, 41, 19, 59, 52]) -> target: 8\n",
      "context: tensor([ 1, 58,  1, 41, 19, 59, 52,  8]) -> target: 58\n",
      "Batch 3\n",
      "context: tensor([56]) -> target: 53\n",
      "context: tensor([56, 53]) -> target: 1\n",
      "context: tensor([56, 53,  1]) -> target: 44\n",
      "context: tensor([56, 53,  1, 44]) -> target: 16\n",
      "context: tensor([56, 53,  1, 44, 16]) -> target: 53\n",
      "context: tensor([56, 53,  1, 44, 16, 53]) -> target: 1\n",
      "context: tensor([56, 53,  1, 44, 16, 53,  1]) -> target: 52\n",
      "context: tensor([56, 53,  1, 44, 16, 53,  1, 52]) -> target: 28\n",
      "Batch 4\n",
      "context: tensor([1]) -> target: 46\n",
      "context: tensor([ 1, 46]) -> target: 53\n",
      "context: tensor([ 1, 46, 53]) -> target: 53\n",
      "context: tensor([ 1, 46, 53, 53]) -> target: 57\n",
      "context: tensor([ 1, 46, 53, 53, 57]) -> target: 10\n",
      "context: tensor([ 1, 46, 53, 53, 57, 10]) -> target: 43\n",
      "context: tensor([ 1, 46, 53, 53, 57, 10, 43]) -> target: 58\n",
      "context: tensor([ 1, 46, 53, 53, 57, 10, 43, 58]) -> target: 56\n"
     ]
    }
   ],
   "source": [
    "# loop through the batches and print the context and target for each batch\n",
    "for b in range(batch_size):\n",
    "    print('Batch {}'.format(b+1))\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print('context: {} -> target: {}'.format(context, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Bigram language model in PyTorch-Lightning\n",
    "class BigramModel(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat.view(-1, self.hparams.vocab_size), y.view(-1))\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat.view(-1, self.hparams.vocab_size), y.view(-1))\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def generate(self, starting_text, length):\n",
    "        with torch.no_grad():\n",
    "            # convert starting_text to tensor\n",
    "            input_seq = encoder(starting_text).unsqueeze(0)\n",
    "            # move input_seq to device\n",
    "            input_seq = input_seq.to(self.device)\n",
    "            # generate sequence of length 'length'\n",
    "            for i in range(length):\n",
    "                # get output probabilities from model\n",
    "                output_probs = self(input_seq)[:,-1,:]\n",
    "                # sample the next token from the output probabilities\n",
    "                next_token = torch.multinomial(F.softmax(output_probs, dim=-1), num_samples=1)\n",
    "                # append the next token to the input sequence\n",
    "                input_seq = torch.cat([input_seq, next_token], dim=1)\n",
    "        # convert the output sequence to text\n",
    "        output_text = ''.join([idx2char[idx] for idx in input_seq.squeeze().tolist()])\n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramModel(\n",
       "  (embedding): Embedding(65, 32)\n",
       "  (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model = BigramModel(vocab_size, embedding_dim=32, hidden_dim=64)\n",
    "bigram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0898,  0.1966, -0.2269,  ...,  0.0039, -0.0763, -0.1268],\n",
       "         [ 0.1250, -0.0110,  0.0631,  ...,  0.0591,  0.2566,  0.0475],\n",
       "         [-0.0113,  0.0902,  0.1115,  ..., -0.0293,  0.0861,  0.1530],\n",
       "         ...,\n",
       "         [ 0.0069,  0.1929, -0.0406,  ...,  0.2338, -0.1185, -0.1099],\n",
       "         [-0.2324, -0.0831, -0.3524,  ..., -0.2395,  0.1335,  0.0672],\n",
       "         [ 0.0411,  0.1028, -0.1139,  ..., -0.1857, -0.0310,  0.0017]],\n",
       "\n",
       "        [[-0.2324, -0.0831, -0.3524,  ..., -0.2395,  0.1335,  0.0672],\n",
       "         [-0.3195,  0.5125, -0.0533,  ...,  0.2048,  0.1758,  0.0952],\n",
       "         [-0.2324, -0.0831, -0.3524,  ..., -0.2395,  0.1335,  0.0672],\n",
       "         ...,\n",
       "         [-0.1921,  0.1451, -0.0171,  ..., -0.1576, -0.1355, -0.0993],\n",
       "         [-0.2719,  0.0547, -0.0394,  ...,  0.0629,  0.0370,  0.3403],\n",
       "         [-0.2136,  0.0358,  0.0807,  ..., -0.3666,  0.1520, -0.0436]],\n",
       "\n",
       "        [[-0.4426, -0.0566,  0.0663,  ...,  0.0090,  0.2780,  0.1964],\n",
       "         [ 0.0708,  0.2102,  0.3248,  ..., -0.0728, -0.2538,  0.2603],\n",
       "         [-0.2324, -0.0831, -0.3524,  ..., -0.2395,  0.1335,  0.0672],\n",
       "         ...,\n",
       "         [ 0.0708,  0.2102,  0.3248,  ..., -0.0728, -0.2538,  0.2603],\n",
       "         [-0.2324, -0.0831, -0.3524,  ..., -0.2395,  0.1335,  0.0672],\n",
       "         [-0.2719,  0.0547, -0.0394,  ...,  0.0629,  0.0370,  0.3403]],\n",
       "\n",
       "        [[-0.2324, -0.0831, -0.3524,  ..., -0.2395,  0.1335,  0.0672],\n",
       "         [-0.0486, -0.0988, -0.4984,  ..., -0.4791,  0.1320,  0.1301],\n",
       "         [ 0.0708,  0.2102,  0.3248,  ..., -0.0728, -0.2538,  0.2603],\n",
       "         ...,\n",
       "         [ 0.1366,  0.0522,  0.0452,  ..., -0.0702,  0.1523,  0.3359],\n",
       "         [-0.2125,  0.1325,  0.1413,  ..., -0.0728, -0.0152, -0.1315],\n",
       "         [-0.3195,  0.5125, -0.0533,  ...,  0.2048,  0.1758,  0.0952]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/pytorch_lightning/core/module.py:420: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.1391, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.training_step((xb, yb), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"HelloFMPdOlgqbqMfgkh,v':okQqg&$XWIK?TEU.kM Zsoh- tTwkWBg\\ndk;XSoKIkSmiF'DjmE'h,OazprsW!C:mLtgWPnDtnVixaZm\\n\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.generate('Hello', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use a larger batch size for actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# create separate train and validation dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 6 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=6)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/ma/miniconda3/envs/myl/lib/python3.10/site-pac ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# define a trainer object\n",
    "trainer = L.Trainer(max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 2.1 K \n",
      "1 | fc1       | Linear    | 2.1 K \n",
      "2 | fc2       | Linear    | 4.2 K \n",
      "----------------------------------------\n",
      "8.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.4 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f768983e0f44f9d8b049a9900fc7e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=71` in the `DataLoader` to improve performance.\n",
      "/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=71` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ab5094ee3649b78628c3fa27a1286c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dc5a220eaf467ebcd828149751d05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.fit(bigram_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': tensor(3.2474), 'val_loss': tensor(3.3175)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the training loss\n",
    "trainer.logged_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helloin eenslorE lnln ezln,isya  iaminN faw t\n",
      "n sou  nuaDb   crei Iwido i \n",
      "e\n",
      "yk  ro w  'ItIirhaiiihvet  e\n"
     ]
    }
   ],
   "source": [
    "input_text = \"hello\"\n",
    "\n",
    "# generate a sequence of length 100\n",
    "print(bigram_model.generate(input_text, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[49,  7, 21, 43,  1, 42,  1, 63],\n",
       "         [ 1, 58,  1, 41, 19, 59, 52,  8],\n",
       "         [56, 53,  1, 44, 16, 53,  1, 52],\n",
       "         [ 1, 46, 53, 53, 57, 10, 43, 58]]),\n",
       " tensor([[ 7, 21, 43,  1, 42,  1, 63, 30],\n",
       "         [58,  1, 41, 19, 59, 52,  8, 58],\n",
       "         [53,  1, 44, 16, 53,  1, 52, 28],\n",
       "         [46, 53, 53, 57, 10, 43, 58, 56]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
