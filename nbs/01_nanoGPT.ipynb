{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt\n",
    "\n",
    "> minimum GPT model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as L \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'data/input.txt'\n",
    "\n",
    "# read the data file\n",
    "with open(DATA_FILE, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115393 characters\n"
     ]
    }
   ],
   "source": [
    "# print the length of the text\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Number of unique characters: 65\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file, as a sorted list.\n",
    "vocab = sorted(set(text))\n",
    "# print out the vocabulary \n",
    "print('Vocabulary: {}'.format(vocab))\n",
    "\n",
    "# the number of unique characters\n",
    "vocab_size = len(vocab)\n",
    "# print the number of unique characters\n",
    "print('Number of unique characters: {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a mapping from characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "# a mapping from indices to characters\n",
    "idx2char = {i:u for i, u in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an encoder function that converts text to a torch tensor\n",
    "def encoder(text):\n",
    "    return torch.tensor([char2idx[c] for c in text], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 43, 50, 50, 53,  1, 27, 50, 47, 60, 47, 43, 56,  2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder('Hello Olivier!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a decoder function that converts a torch tensor to text\n",
    "def decoder(tensor):\n",
    "    return ''.join([idx2char[i.item()] for i in tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Olivier!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(encoder('Hello Olivier!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a bigger vocabulary would normally imply a shorter encoding length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentencePiece BPE tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56])\n"
     ]
    }
   ],
   "source": [
    "# encode the whole text\n",
    "encoded_text = encoder(text)\n",
    "\n",
    "# print the encoded text (first 500 characters)\n",
    "print(encoded_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(decoder(encoded_text[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/validation split (90/10)\n",
    "train_size = int(0.9 * len(encoded_text))\n",
    "val_size = len(encoded_text) - train_size\n",
    "train_text, val_text = torch.utils.data.random_split(encoded_text, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the block size to 8 \n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and visualise how the self-supervised model do its predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([57, 56, 47, 40, 57, 43, 58, 52]),\n",
       " tensor([56, 47, 40, 57, 43, 58, 52, 53]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_text[0:block_size]\n",
    "y = train_text[1:block_size+1]\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next token prediction\n",
    "\n",
    "the model looks at all the previous tokens, up to the length of the block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: tensor([57]) -> target: 56\n",
      "context: tensor([57, 56]) -> target: 47\n",
      "context: tensor([57, 56, 47]) -> target: 40\n",
      "context: tensor([57, 56, 47, 40]) -> target: 57\n",
      "context: tensor([57, 56, 47, 40, 57]) -> target: 43\n",
      "context: tensor([57, 56, 47, 40, 57, 43]) -> target: 58\n",
      "context: tensor([57, 56, 47, 40, 57, 43, 58]) -> target: 52\n",
      "context: tensor([57, 56, 47, 40, 57, 43, 58, 52]) -> target: 53\n"
     ]
    }
   ],
   "source": [
    "# show the context and target for all the training examples\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print('context: {} -> target: {}'.format(context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, block_size=8):\n",
    "        self.text = text\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text) // self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get a block of size block_size starting from index idx * block_size\n",
    "        start_idx = idx * self.block_size\n",
    "        end_idx = start_idx + self.block_size\n",
    "        data = self.text[start_idx:end_idx]\n",
    "        target = self.text[start_idx+1:end_idx+1]\n",
    "        return data, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate train and validation datasets\n",
    "train_dataset = TextDataset(train_text, block_size=block_size)\n",
    "val_dataset = TextDataset(val_text, block_size=block_size)\n",
    "\n",
    "# create separate train and validation dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[52, 47, 56, 13,  1,  1, 57, 51],\n",
       "         [27, 53, 52, 51,  1, 47,  5,  6],\n",
       "         [ 1,  6, 46, 58, 46, 52, 51, 52],\n",
       "         [15, 56, 41, 56, 63, 46, 39, 57]]),\n",
       " tensor([[47, 56, 13,  1,  1, 57, 51, 57],\n",
       "         [53, 52, 51,  1, 47,  5,  6, 59],\n",
       "         [ 6, 46, 58, 46, 52, 51, 52, 53],\n",
       "         [56, 41, 56, 63, 46, 39, 57, 58]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dataloader))\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "context: tensor([52]) -> target: 47\n",
      "context: tensor([52, 47]) -> target: 56\n",
      "context: tensor([52, 47, 56]) -> target: 13\n",
      "context: tensor([52, 47, 56, 13]) -> target: 1\n",
      "context: tensor([52, 47, 56, 13,  1]) -> target: 1\n",
      "context: tensor([52, 47, 56, 13,  1,  1]) -> target: 57\n",
      "context: tensor([52, 47, 56, 13,  1,  1, 57]) -> target: 51\n",
      "context: tensor([52, 47, 56, 13,  1,  1, 57, 51]) -> target: 57\n",
      "Batch 2\n",
      "context: tensor([27]) -> target: 53\n",
      "context: tensor([27, 53]) -> target: 52\n",
      "context: tensor([27, 53, 52]) -> target: 51\n",
      "context: tensor([27, 53, 52, 51]) -> target: 1\n",
      "context: tensor([27, 53, 52, 51,  1]) -> target: 47\n",
      "context: tensor([27, 53, 52, 51,  1, 47]) -> target: 5\n",
      "context: tensor([27, 53, 52, 51,  1, 47,  5]) -> target: 6\n",
      "context: tensor([27, 53, 52, 51,  1, 47,  5,  6]) -> target: 59\n",
      "Batch 3\n",
      "context: tensor([1]) -> target: 6\n",
      "context: tensor([1, 6]) -> target: 46\n",
      "context: tensor([ 1,  6, 46]) -> target: 58\n",
      "context: tensor([ 1,  6, 46, 58]) -> target: 46\n",
      "context: tensor([ 1,  6, 46, 58, 46]) -> target: 52\n",
      "context: tensor([ 1,  6, 46, 58, 46, 52]) -> target: 51\n",
      "context: tensor([ 1,  6, 46, 58, 46, 52, 51]) -> target: 52\n",
      "context: tensor([ 1,  6, 46, 58, 46, 52, 51, 52]) -> target: 53\n",
      "Batch 4\n",
      "context: tensor([15]) -> target: 56\n",
      "context: tensor([15, 56]) -> target: 41\n",
      "context: tensor([15, 56, 41]) -> target: 56\n",
      "context: tensor([15, 56, 41, 56]) -> target: 63\n",
      "context: tensor([15, 56, 41, 56, 63]) -> target: 46\n",
      "context: tensor([15, 56, 41, 56, 63, 46]) -> target: 39\n",
      "context: tensor([15, 56, 41, 56, 63, 46, 39]) -> target: 57\n",
      "context: tensor([15, 56, 41, 56, 63, 46, 39, 57]) -> target: 58\n"
     ]
    }
   ],
   "source": [
    "# loop through the batches and print the context and target for each batch\n",
    "for b in range(batch_size):\n",
    "    print('Batch {}'.format(b+1))\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print('context: {} -> target: {}'.format(context, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Bigram language model in PyTorch-Lightning\n",
    "class BigramModel(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat.view(-1, self.hparams.vocab_size), y.view(-1))\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat.view(-1, self.hparams.vocab_size), y.view(-1))\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def generate(self, starting_text, length):\n",
    "        with torch.no_grad():\n",
    "            # convert starting_text to tensor\n",
    "            input_seq = encoder(starting_text).unsqueeze(0)\n",
    "            # move input_seq to device\n",
    "            input_seq = input_seq.to(self.device)\n",
    "            # generate sequence of length 'length'\n",
    "            for i in range(length):\n",
    "                # get output probabilities from model\n",
    "                output_probs = self(input_seq)[:,-1,:]\n",
    "                # sample the next token from the output probabilities\n",
    "                next_token = torch.multinomial(F.softmax(output_probs, dim=-1), num_samples=1)\n",
    "                # append the next token to the input sequence\n",
    "                input_seq = torch.cat([input_seq, next_token], dim=1)\n",
    "        # convert the output sequence to text\n",
    "        output_text = ''.join([idx2char[idx] for idx in input_seq.squeeze().tolist()])\n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramModel(\n",
       "  (embedding): Embedding(65, 32)\n",
       "  (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model = BigramModel(vocab_size, embedding_dim=32, hidden_dim=64)\n",
    "bigram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2678, -0.1369, -0.4027,  ...,  0.1395, -0.1126,  0.1175],\n",
       "         [ 0.4385, -0.5030,  0.1241,  ...,  0.4192, -0.2588,  0.2549],\n",
       "         [ 0.0025, -0.2363, -0.2232,  ...,  0.1668,  0.0379,  0.1697],\n",
       "         ...,\n",
       "         [ 0.0095, -0.2617, -0.1210,  ..., -0.1893,  0.0417,  0.1678],\n",
       "         [-0.0532, -0.2724, -0.2155,  ...,  0.1709, -0.3195,  0.1353],\n",
       "         [ 0.2438, -0.5204, -0.4409,  ...,  0.0225,  0.1857, -0.2476]],\n",
       "\n",
       "        [[-0.0637, -0.1605, -0.2607,  ..., -0.0072, -0.1926,  0.1827],\n",
       "         [-0.0352, -0.1897, -0.2359,  ...,  0.5171, -0.0788,  0.3116],\n",
       "         [ 0.2678, -0.1369, -0.4027,  ...,  0.1395, -0.1126,  0.1175],\n",
       "         ...,\n",
       "         [ 0.4385, -0.5030,  0.1241,  ...,  0.4192, -0.2588,  0.2549],\n",
       "         [ 0.0431, -0.1951, -0.1140,  ...,  0.1812, -0.2083,  0.1417],\n",
       "         [ 0.1701, -0.4269,  0.0718,  ...,  0.2989,  0.0870,  0.3772]],\n",
       "\n",
       "        [[ 0.0095, -0.2617, -0.1210,  ..., -0.1893,  0.0417,  0.1678],\n",
       "         [ 0.1701, -0.4269,  0.0718,  ...,  0.2989,  0.0870,  0.3772],\n",
       "         [-0.0859, -0.2851, -0.2889,  ...,  0.4188,  0.2584,  0.0595],\n",
       "         ...,\n",
       "         [ 0.2678, -0.1369, -0.4027,  ...,  0.1395, -0.1126,  0.1175],\n",
       "         [ 0.2438, -0.5204, -0.4409,  ...,  0.0225,  0.1857, -0.2476],\n",
       "         [ 0.2678, -0.1369, -0.4027,  ...,  0.1395, -0.1126,  0.1175]],\n",
       "\n",
       "        [[ 0.3378, -0.5768,  0.0498,  ...,  0.4863,  0.2094,  0.1375],\n",
       "         [ 0.0025, -0.2363, -0.2232,  ...,  0.1668,  0.0379,  0.1697],\n",
       "         [ 0.1970, -0.5068, -0.1732,  ...,  0.2811,  0.0542, -0.2996],\n",
       "         ...,\n",
       "         [-0.0859, -0.2851, -0.2889,  ...,  0.4188,  0.2584,  0.0595],\n",
       "         [ 0.1381, -0.1962,  0.4191,  ...,  0.1700, -0.0942,  0.1710],\n",
       "         [-0.0532, -0.2724, -0.2155,  ...,  0.1709, -0.3195,  0.1353]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/pytorch_lightning/core/module.py:420: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.2066, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.training_step((xb, yb), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello$Y.t3YVnmOxkYsBQqGl,ulvlAZywHMkHfTYQypbWpADSYM'dvU.,:aNlpOpezpWqwm&!!Usw&hf?cGsBBI\\nyaG:VUXdmCFuvbdmO\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.generate('Hello', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use a larger batch size for actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# create separate train and validation dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 6 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=6)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/ma/miniconda3/envs/myl/lib/python3.10/site-pac ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# define a trainer object\n",
    "trainer = L.Trainer(max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 2.1 K \n",
      "1 | fc1       | Linear    | 2.1 K \n",
      "2 | fc2       | Linear    | 4.2 K \n",
      "----------------------------------------\n",
      "8.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.4 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8804654dec431191d186f88c6c19d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=71` in the `DataLoader` to improve performance.\n",
      "/home/ma/miniconda3/envs/myl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=71` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93585c72bdf14d94ae01ef5ff1968100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d44986c36341e4a0f004d38ac83038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.fit(bigram_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': tensor(3.2826), 'val_loss': tensor(3.3157)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the training loss\n",
    "trainer.logged_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helloeoe eS\n",
      "\n",
      "i uhm: S awh?mTt  OuOfene  n l\n",
      ",dt dheoggn neter \n",
      "  etoounneweiuy K r  oH th ei amcnnhrL o t\n"
     ]
    }
   ],
   "source": [
    "input_text = \"hello\"\n",
    "\n",
    "# generate a sequence of length 100\n",
    "print(bigram_model.generate(input_text, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1302,  0.6888, -1.1523, -1.2340,  1.1439],\n",
       "        [ 0.2148,  0.0503, -2.5361,  0.5335, -1.0008],\n",
       "        [-0.0530,  0.7277,  1.4551,  0.4140,  0.5039],\n",
       "        [ 0.4920, -0.9831, -1.5567,  0.4762, -0.5163]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use masked matrix multiplication to compute cumulative averages\n",
    "T, E = 4, 5  # sequence length, embedding dimension\n",
    "x = torch.randn(T, E)  # input tensor\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(T, T)) # mask matrix\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# initialize the weights with zeros\n",
    "weights = torch.zeros(T, T)\n",
    "\n",
    "# replace the upper triangular part of the weights with -inf\n",
    "weights = weights.masked_fill(mask==0, float('-inf'))\n",
    "\n",
    "# apply softmax to the weights\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1302,  0.6888, -1.1523, -1.2340,  1.1439],\n",
       "        [-0.4577,  0.3696, -1.8442, -0.3503,  0.0715],\n",
       "        [-0.3228,  0.4890, -0.7444, -0.0955,  0.2157],\n",
       "        [-0.1191,  0.1209, -0.9475,  0.0474,  0.0327]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move on to the attention model by using an affinity based weighing scheme\n",
    "\n",
    "the affinity will be calculated by the dot product of the query and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define queries, keys, and values using linear layers\n",
    "H = 8\n",
    "key = nn.Linear(E, H, bias=False)\n",
    "query = nn.Linear(E, H, bias=False)\n",
    "value = nn.Linear(E, H, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0243, -0.7339,  0.1138,  1.1829, -0.2428, -0.3732,  0.4777,  1.2436],\n",
       "        [ 0.5204,  0.0916,  0.1016, -0.2374,  0.6966,  0.3451,  1.1526,  0.1647],\n",
       "        [-0.5639, -0.0049,  0.0393, -0.2037, -0.5386, -0.3055, -0.6389, -0.3777],\n",
       "        [ 0.5188,  0.2483, -0.4000, -0.3071,  0.6248,  0.7772,  0.4243,  0.0901]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=8, bias=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8429, -0.2807, -0.5572,  0.4662, -0.2232, -1.7832, -0.7288, -0.2520],\n",
       "        [-0.5455, -1.0265,  0.9197, -1.1817, -1.0555, -0.2829, -0.4550,  1.0635],\n",
       "        [ 0.8573,  0.9310, -0.7485,  0.2056,  0.8744,  0.2372,  0.5925, -0.4311],\n",
       "        [-0.4999, -0.8215,  0.9373, -0.6269, -0.9207,  0.2695, -0.5260,  0.6619]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2501, 0.7499, 0.0000, 0.0000],\n",
       "        [0.5914, 0.3762, 0.0324, 0.0000],\n",
       "        [0.0145, 0.0312, 0.9104, 0.0440]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = query(x) @ key(x).transpose(-1, -2)\n",
    "weights = weights.masked_fill(mask==0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1302,  0.6888, -1.1523, -1.2340,  1.1439],\n",
       "        [-0.1216,  0.2101, -2.1900,  0.0914, -0.4644],\n",
       "        [-0.5893,  0.4499, -1.5884, -0.5156,  0.3162],\n",
       "        [-0.0364,  0.6308,  1.1606,  0.3965,  0.4215]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights @ x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
